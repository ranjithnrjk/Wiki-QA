[
  {
    "objectID": "mini_wiki_demo.html",
    "href": "mini_wiki_demo.html",
    "title": "Mini Wiki-QA ğŸ¤–",
    "section": "",
    "text": "This is a small demo app built with Gradio and deployed on Hugging Face Spaces.\nIt answers simple questions based on a small given context â€” like a mini open-domain QA bot."
  },
  {
    "objectID": "mini_wiki_demo.html#project-summary",
    "href": "mini_wiki_demo.html#project-summary",
    "title": "Mini Wiki-QA ğŸ¤–",
    "section": "ğŸ§  Project Summary",
    "text": "ğŸ§  Project Summary\nThis demo is part of my learning path toward Retrieval-Augmented Generation (RAG) systems.\nIn a full RAG chatbot: 1. The userâ€™s query is first embedded into a vector space. 2. Relevant documents are retrieved from a vector database (e.g., Chroma). 3. A language model (LLM) generates an answer using the retrieved context.\nHere, Iâ€™m focusing on Step 3 â€” using a Hugging Face LLM API (flan-t5-small) to generate responses based on a short context."
  },
  {
    "objectID": "mini_wiki_demo.html#how-it-works",
    "href": "mini_wiki_demo.html#how-it-works",
    "title": "Mini Wiki-QA ğŸ¤–",
    "section": "ğŸ§© How It Works",
    "text": "ğŸ§© How It Works\n\nA fixed context is defined:\nPython is a programming language created by Guido van Rossum. It emphasizes readability and is widely used in data science, web development, and automation.\nThe user enters a question.\nThe app sends this to the Hugging Face model via the Inference API.\nThe model generates a response."
  },
  {
    "objectID": "mini_wiki_demo.html#try-it-out",
    "href": "mini_wiki_demo.html#try-it-out",
    "title": "Mini Wiki-QA ğŸ¤–",
    "section": "ğŸš€ Try It Out",
    "text": "ğŸš€ Try It Out\n\n\nOpen Demo on Hugging Face ğŸš€\n\nIf youâ€™re viewing this locally and the demo doesnâ€™t load,\nyou can open it directly here ğŸ‘‰\nLaunch on Hugging Face Spaces"
  },
  {
    "objectID": "mini_wiki_demo.html#tech-stack",
    "href": "mini_wiki_demo.html#tech-stack",
    "title": "Mini Wiki-QA ğŸ¤–",
    "section": "ğŸ› ï¸ Tech Stack",
    "text": "ğŸ› ï¸ Tech Stack\n\nFrontend/UI: Gradio\n\nModel API: Hugging Face Inference API\n\nHosting: Hugging Face Spaces\n\nDocumentation Site: Quarto + GitHub Pages"
  },
  {
    "objectID": "mini_wiki_demo.html#next-steps",
    "href": "mini_wiki_demo.html#next-steps",
    "title": "Mini Wiki-QA ğŸ¤–",
    "section": "ğŸ’¡ Next Steps",
    "text": "ğŸ’¡ Next Steps\n\nReplace static context with dynamic document retrieval using Chroma or FAISS.\n\nIntegrate a larger model (e.g., mistral-7b or gemma-2b).\n\nAdd RAG pipeline â†’ full contextual chatbot experience."
  },
  {
    "objectID": "mini_wiki_demo.html#learnings",
    "href": "mini_wiki_demo.html#learnings",
    "title": "Mini Wiki-QA ğŸ¤–",
    "section": "ğŸ“š Learnings",
    "text": "ğŸ“š Learnings\nThis project helped me understand: - Using Hugging Face APIs for lightweight inference\n- Creating shareable ML demos without custom web dev\n- Embedding such demos in a personal portfolio using Quarto\n\nBuilt with â¤ï¸ by Ranjith Â· Powered by Hugging Face, Quarto, and GitHub Pages"
  }
]